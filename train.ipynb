{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "fake_filepath = r\"C:\\Users\\vm658\\Documents\\SVSM\\Research\\Code\\DataSet_Misinfo_FAKE.csv\"\n",
    "true_filepath = r\"C:\\Users\\vm658\\Documents\\SVSM\\Research\\Code\\DataSet_Misinfo_TRUE.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df = pd.read_csv(fake_filepath)\n",
    "fake_df['label'] = 0\n",
    "#remove first column\n",
    "fake_df = fake_df.drop(fake_df.columns[0], axis=1)\n",
    "#drop empty rows\n",
    "fake_df = fake_df.dropna(how = 'any')\n",
    "fake_df.drop_duplicates(subset = ['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df = pd.read_csv(true_filepath, encoding = 'latin-1')\n",
    "true_df['label'] = 1\n",
    "#remove first column\n",
    "true_df = true_df.drop(true_df.columns[0], axis=1)\n",
    "#drop empty rows\n",
    "true_df = true_df.dropna(how = 'any')\n",
    "true_df.drop_duplicates(subset = ['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([true_df,fake_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "#add more punctuation\n",
    "punctuations = string.punctuation + \"’‘“”\"\n",
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    no_punct = \"\".join([c for c in text if c not in punctuations])\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "df['text'] = df['text'].apply(lambda x: remove_punctuation(x))\n",
    "#make all text lowercase\n",
    "df['text'] = df['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data for training and testing\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(df['text'],df['label'],test_size=0.2, random_state = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(r\"C:\\Users\\vm658\\Documents\\SVSM\\Research\\Code\\fake_or_real_news.csv\")\n",
    "\n",
    "#relabel fake and true data as 0 and 1\n",
    "test_df['label'] = test_df.label.map({'FAKE': 0, 'REAL': 1})\n",
    "\n",
    "test_df = test_df.drop(['Unnamed: 0'], axis=1)\n",
    "test_df = test_df.dropna(how = 'any')\n",
    "test_df.drop_duplicates(subset = ['title'], inplace=True)\n",
    "\n",
    "#remove punctuation\n",
    "test_df['title'] = test_df['title'].apply(lambda x: remove_punctuation(x))\n",
    "#make all text lowercase\n",
    "#test_df['title'] = test_df['title'].apply(lambda x: x.lower())\n",
    "\n",
    "\n",
    "#remove punctuation\n",
    "test_df['text'] = test_df['text'].apply(lambda x: remove_punctuation(x))\n",
    "#make all text lowercase\n",
    "test_df['text'] = test_df['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vm658\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vm658\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['said', \"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'])\n",
    "stop_words.extend(['is', 'it', 'to', 'be', 'said', 'he', 'one', 'that', 'also', 'in', 'this', 'are', 'an', 'you', 'they'])\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''cmaps = {'Train_data': 'Greys',\n",
    "         'Test_titles': 'GnBu',\n",
    "         'Test_text': 'Wistia'}'''\n",
    "\n",
    "def test(model, algorithm_name, x_test, y_test, cmap):\n",
    "    #test the model\n",
    "    prediction = model.predict(x_test)\n",
    "    score = metrics.accuracy_score(y_test, prediction)\n",
    "    print(f\"{algorithm_name} accuracy:   %0.3f\" % (score*100))\n",
    "    print(f'{algorithm_name}\\n',classification_report(y_test, prediction, target_names=['Fake','True']))\n",
    "    cm = metrics.confusion_matrix(y_test, prediction, labels=[0,1])\n",
    "\n",
    "    fig, ax = plot_confusion_matrix(conf_mat=confusion_matrix(y_test, prediction),\n",
    "                                    show_absolute=True,\n",
    "                                    show_normed=True,\n",
    "                                    colorbar=True,\n",
    "                                    cmap = cmap)\n",
    "    ax.set_title(f'{algorithm_name} Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vm658\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Multinomial NB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range = (2,2), tokenizer=word_tokenize, stop_words=stop_words)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "MNB_model = pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "filename = 'Multinomial_NB.sav'\n",
    "pickle.dump(MNB_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vm658\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range = (2,2), tokenizer=word_tokenize, stop_words=stop_words)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "SVM_model = pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "filename = 'SVM.sav'\n",
    "pickle.dump(SVM_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vm658\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range = (2,2), tokenizer = word_tokenize, stop_words=stop_words)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "LR_model = pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "filename = 'Logistic_Regression.sav'\n",
    "pickle.dump(LR_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Aggressive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vm658\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Passive Aggressive Classifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range = (2,2), tokenizer = word_tokenize, stop_words = stop_words)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf',  PassiveAggressiveClassifier())\n",
    "])\n",
    "\n",
    "PAC_model = pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "filename = 'PAC.sav'\n",
    "pickle.dump(PAC_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vm658\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range = (2,2), tokenizer = word_tokenize, stop_words = stop_words)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf',  RandomForestClassifier(random_state = 0, n_estimators = 200))\n",
    "])\n",
    "\n",
    "RF_model = pipe.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "filename = 'Random_Forest.sav'\n",
    "pickle.dump(RF_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vm658\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#XG Boost\n",
    "import xgboost as xg\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range = (2,2), tokenizer = word_tokenize, stop_words = stop_words)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', xg.XGBClassifier(objective = 'binary:logistic', n_estimators = 200, seed = 123))\n",
    "])\n",
    "\n",
    "XGB_model = pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#save the model\n",
    "filename = 'XG_Boost.sav'\n",
    "pickle.dump(XGB_model, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
